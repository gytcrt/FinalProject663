{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Implementation - Variational Inference\n",
    "\n",
    "Latent Dirichlet Allocation was also implemented using variational inference. In situations where variational inference is typically used, the posterior is typically intractable to calculate directly. In the case of LDA, the posterior $p(\\theta,z,w | \\alpha,\\beta)$ is difficult to compute, so the distribution is instead approximated with the variational distribution:\n",
    "\n",
    "$$q(\\theta,z | \\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^{N} q(z_n|\\phi_n)$$\n",
    "\n",
    "Using Jensen's inequality, it can be shown that the difference between the log likelihood of the true posterior and the variational approximation is the KL-divergence between the two. In order words:\n",
    "\n",
    "$$ \\log(p(w|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,z|\\gamma,\\phi)||p(\\theta,z|w,\\alpha,\\beta))$$\n",
    "\n",
    "We can choose to either minimize the KL-divergence or maximize the likelihood. Here, the latter is approach is taken. Factoring the likelihood appropriately, we can write the following:\n",
    "\n",
    "$$ L(\\gamma,\\phi;\\alpha,\\beta) = E_q[\\log p(\\theta|\\alpha)] + E_q[\\log p(z|\\theta)] + E_q [\\log p(w|z,\\beta)] - E_q [\\log q(\\theta)] - E_q[\\log q(z)] $$\n",
    "\n",
    "This likelihood is maximized through Expectation-Maximization (EM). During the expectation step, the variational parameters $\\phi$ and $\\gamma$ are first optimized by maximizing the likelihood with respect to each individually. During the maximization step, the likelihood is then maximized with respect to model parameters $\\alpha$ and $\\beta$. This process is outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic:       $z = 1,...,k$\n",
    "\n",
    "word:        $w = 1,...,N_m$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "$\\alpha: 1 \\times k$ Model parameter - vector of topic distribution probabilities for each document\n",
    "\n",
    "$\\beta: k \\times v$ Model parameter - matrix of word probabilities for each topic\n",
    "\n",
    "$\\phi: M \\times N_m \\times k$ Variational parameter - matrix of topic probabilities for each word in each document\n",
    "\n",
    "$\\gamma: M \\times k$ Variational parameter - matrix of topic probabilities for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt,mean,square\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize variational parameters $\\phi$ and $\\gamma$\n",
    "\n",
    "By taking the derivative the log likelihood with respect to $\\phi$ and setting the result to zero, we find the maximal value of $\\phi$:\n",
    "\n",
    "$$ \\phi_{ni} \\propto \\beta_{iv} \\exp(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k(\\gamma_j)) $$\n",
    "\n",
    "where $\\beta_{iv}$ = $p(w_n^v = 1|z_n = i)$ and $\\psi$ is the digamma function (derivative of the log gamma function $\\Gamma$). As $\\phi$ represents the probability of each word in a document for each state, these values must be normalized such that each row representing a word position within a document must sum to 1.\n",
    "\n",
    "\n",
    "In a similar fashion, it can be shown that $\\gamma$ is maximized at:\n",
    "\n",
    "$$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N(\\phi_{ni})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize variational parameter phi\n",
    "def opt_phi(beta,gamma,words,M,N,k):\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(k):\n",
    "                phi[m][n,i] = beta[words[m][n],i] * np.exp(digamma(gamma[m,i]) - digamma(np.sum(gamma[m,:])))\n",
    "            # Normalize across states so phi represents probability over states for each word\n",
    "            phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "    return phi\n",
    "\n",
    "\n",
    "## Optimize variational parameter gamma\n",
    "def opt_gamma(alpha,phi,M):\n",
    "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi)))\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters $\\alpha$ and $\\beta$\n",
    "\n",
    "By taking the derivative of the log likelihood and applying the appropriate Lagrange multipliers to ensure probabilities sum to 1, we find that $/beta$ is maximized with:\n",
    "\n",
    "$$ \\beta_{ij} \\propto \\sum_{m=1}^M \\sum_{n=1}^{N_m} \\phi_{dni}w_{mn}^j$$\n",
    "\n",
    "where $w_{mn}^j$ = 1 if the $n^{th}$ word of document $m$ is equal to $j$, and 0 otherwise. Since the columns of \\beta represent the probability of each word given the topic of that particular column, they must be normalized to sum to 1.\n",
    "\n",
    "Taking the derivative of the log likelihood with respect to $\\alpha$ yields:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial\\alpha_i} = M(\\Psi(\\sum_{j=1}^k\\alpha_j)-\\Psi(\\alpha_i)) - \\sum_{m=1}^M(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}))$$\n",
    "\n",
    "Because this is difficult to find the zero intercept of this derivative, $\\alpha$ is instead maximized numerically with the Newton-Raphson method. The Hessian is of the form:\n",
    "\n",
    "$$ \\frac{\\partial^2 L}{\\partial\\alpha_i\\partial\\alpha_j} = M(\\Psi'(\\sum_{j=1}^k \\alpha_j) - \\delta(i,j)\\Psi'(\\alpha_i))$$ \n",
    "\n",
    "Note: This is slightly different from what is stated in the paper, which has a couple errors in the reported form of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize beta\n",
    "def est_beta(phi,words,k,V):\n",
    "    for j in range (V):\n",
    "        # Construct w_mn == j of same shape as phi\n",
    "        w_mnj = [np.tile((word == j),(k,1)).T for word in words]\n",
    "        beta[j,:] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))),axis=0)\n",
    "        \n",
    "    # Normalize across states so beta represents probability of each word given the state\n",
    "    for i in range(k):\n",
    "        beta[:,i] = beta[:,i]/sum(beta[:,i])\n",
    "        \n",
    "    return beta\n",
    "\n",
    "\n",
    "## Optimize alpha\n",
    "#  (Newton-Raphson method, for a Hessian with special structure)\n",
    "def est_alpha(alpha,gamma,M,k,nr_max_iters = 1000,tol = 10**-2.0):\n",
    "    for it in range(nr_max_iters):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        #  Calculate gradient \n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) + np.sum(digamma(gamma)-np.tile(digamma(np.sum(gamma,axis=1)),(k,1)).T,axis=0)\n",
    "        #  Calculate Hessian diagonal component\n",
    "        h = -M*polygamma(1,alpha) \n",
    "        #  Calculate Hessian constant component\n",
    "        z = M*polygamma(1,np.sum(alpha))\n",
    "        #  Calculate constant\n",
    "        c = np.sum(g/h)/(z**(-1.0)+np.sum(h**(-1.0)))\n",
    "\n",
    "        #  Update alpha\n",
    "        alpha = alpha - (g-c)/h\n",
    "        \n",
    "        #  Check convergence\n",
    "        if sqrt(mean(square(alpha-alpha_old)))<tol:\n",
    "            break\n",
    "        \n",
    "    return alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
