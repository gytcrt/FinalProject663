{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Implementation - Variational Inference\n",
    "\n",
    "Latent Dirichlet Allocation was also implemented using variational inference. In situations where variational inference is typically used, the posterior is typically intractable to calculate directly. In the case of LDA, the posterior $p(\\theta,z,w | \\alpha,\\beta)$ is difficult to compute, so the distribution is instead approximated with the variational distribution:\n",
    "\n",
    "$$q(\\theta,z | \\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^{N} q(z_n|\\phi_n)$$\n",
    "\n",
    "Using Jensen's inequality, it can be shown that the difference between the log likelihood of the true posterior and the variational approximation is the KL-divergence between the two. In order words:\n",
    "\n",
    "$$ \\log(p(w|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,z|\\gamma,\\phi)||p(\\theta,z|w,\\alpha,\\beta))$$\n",
    "\n",
    "We can choose to either minimize the KL-divergence or maximize the likelihood. Here, the latter is approach is taken. Factoring the likelihood appropriately, we can write the following:\n",
    "\n",
    "$$ L(\\gamma,\\phi;\\alpha,\\beta) = E_q[\\log p(\\theta|\\alpha)] + E_q[\\log p(z|\\theta)] + E_q [\\log p(w|z,\\beta)] - E_q [\\log q(\\theta)] - E_q[\\log q(z)] $$\n",
    "\n",
    "This likelihood is maximized through Expectation-Maximization (EM). During the expectation step, the variational parameters $\\phi$ and $\\gamma$ are first optimized by maximizing the likelihood with respect to each individually. During the maximization step, the likelihood is then maximized with respect to model parameters $\\alpha$ and $\\beta$. This process is outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic:       $z = 1,...,k$\n",
    "\n",
    "word:        $w = 1,...,N_m$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "$\\alpha: 1 \\times k$ Model parameter - vector of topic distribution probabilities for each document\n",
    "\n",
    "$\\beta: k \\times v$ Model parameter - matrix of word probabilities for each topic\n",
    "\n",
    "$\\phi: M \\times N_m \\times k$ Variational parameter - matrix of topic probabilities for each word in each document\n",
    "\n",
    "$\\gamma: M \\times k$ Variational parameter - matrix of topic probabilities for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt,mean,square\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize variational parameters $\\phi$ and $\\gamma$\n",
    "\n",
    "By taking the derivative the log likelihood with respect to $\\phi$ and setting the result to zero, we find the maximal value of $\\phi$:\n",
    "\n",
    "$$ \\phi_{ni} \\propto \\beta_{iv} \\exp(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k(\\gamma_j)) $$\n",
    "\n",
    "where $\\beta_{iv}$ = $p(w_n^v = 1|z_n = i)$ and $\\psi$ is the digamma function (derivative of the log gamma function $\\Gamma$). As $\\phi$ represents the probability of each word in a document for each state, these values must be normalized such that each row representing a word position within a document must sum to 1.\n",
    "\n",
    "\n",
    "In a similar fashion, it can be shown that $\\gamma$ is maximized at:\n",
    "\n",
    "$$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N(\\phi_{ni})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize variational parameter phi\n",
    "def opt_phi(beta,gamma,words,M,N,k):\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(k):\n",
    "                phi[m][n,i] = beta[words[m][n],i] * np.exp(digamma(gamma[m,i]) - digamma(np.sum(gamma[m,:])))\n",
    "            # Normalize across states so phi represents probability over states for each word\n",
    "            phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "    return phi\n",
    "\n",
    "\n",
    "## Optimize variational parameter gamma\n",
    "def opt_gamma(alpha,phi,M):\n",
    "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi)))\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters $\\alpha$ and $\\beta$\n",
    "\n",
    "By taking the derivative of the log likelihood and applying the appropriate Lagrange multipliers to ensure probabilities sum to 1, we find that $/beta$ is maximized with:\n",
    "\n",
    "$$ \\beta_{ij} \\propto \\sum_{m=1}^M \\sum_{n=1}^{N_m} \\phi_{dni}w_{mn}^j$$\n",
    "\n",
    "where $w_{mn}^j$ = 1 if the $n^{th}$ word of document $m$ is equal to $j$, and 0 otherwise. Since the columns of \\beta represent the probability of each word given the topic of that particular column, they must be normalized to sum to 1.\n",
    "\n",
    "Taking the derivative of the log likelihood with respect to $\\alpha$ yields:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial\\alpha_i} = M(\\Psi(\\sum_{j=1}^k\\alpha_j)-\\Psi(\\alpha_i)) - \\sum_{m=1}^M(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}))$$\n",
    "\n",
    "Because this is difficult to find the zero intercept of this derivative, $\\alpha$ is instead maximized numerically with the Newton-Raphson method. The Hessian is of the form:\n",
    "\n",
    "$$ \\frac{\\partial^2 L}{\\partial\\alpha_i\\partial\\alpha_j} = M(\\Psi'(\\sum_{j=1}^k \\alpha_j) - \\delta(i,j)\\Psi'(\\alpha_i))$$ \n",
    "\n",
    "Note: This is slightly different from what is stated in the paper, which has a couple errors in the reported form of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize beta\n",
    "def est_beta(phi,words,k,V):\n",
    "    for j in range (V):\n",
    "        # Construct w_mn == j of same shape as phi\n",
    "        w_mnj = [np.tile((word == j),(k,1)).T for word in words]\n",
    "        beta[j,:] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))),axis=0)\n",
    "        \n",
    "    # Normalize across states so beta represents probability of each word given the state\n",
    "    for i in range(k):\n",
    "        beta[:,i] = beta[:,i]/sum(beta[:,i])\n",
    "        \n",
    "    return beta\n",
    "\n",
    "\n",
    "## Optimize alpha\n",
    "#  (Newton-Raphson method, for a Hessian with special structure)\n",
    "def est_alpha(alpha,gamma,M,k,nr_max_iters = 1000,tol = 10**-2.0):\n",
    "    for it in range(nr_max_iters):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        #  Calculate gradient \n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) + np.sum(digamma(gamma)-np.tile(digamma(np.sum(gamma,axis=1)),(k,1)).T,axis=0)\n",
    "        #  Calculate Hessian diagonal component\n",
    "        h = -M*polygamma(1,alpha) \n",
    "        #  Calculate Hessian constant component\n",
    "        z = M*polygamma(1,np.sum(alpha))\n",
    "        #  Calculate constant\n",
    "        c = np.sum(g/h)/(z**(-1.0)+np.sum(h**(-1.0)))\n",
    "\n",
    "        #  Update alpha\n",
    "        alpha = alpha - (g-c)/h\n",
    "        \n",
    "        #  Check convergence\n",
    "        if sqrt(mean(square(alpha-alpha_old)))<tol:\n",
    "            break\n",
    "        \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Tests - Variational Inference \n",
    "\n",
    "### Set data set characteristics\n",
    "To test if our implementation of latent dirichlet allocation with variational inference works, we first generate some toy data. This toy data set will consist of 300 documents, each with a uniform random length between 150 and 200 words. The size of the vocabulary of words in the documents is set to be 30, assumed to be generated from 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: [173 178 190 189 175 189 176 168 170 158 159 156 176 173 174 151 177 179\n",
      " 156 172 152 191 190 161 151 173 169 196 167 197 177 153 170 158 158 157\n",
      " 177 159 154 181 183 162 156 154 196 170 168 176 153 191 184 192 154 158\n",
      " 164 188 153 174 179 158 197 157 154 185 168 159 199 178 163 162 198 195\n",
      " 183 195 150 199 159 170 195 184 198 198 177 157 170 171 188 194 150 166\n",
      " 168 155 191 175 198 179 173 169 156 160 195 160 195 166 177 177 153 191\n",
      " 162 195 165 150 162 157 161 151 188 183 190 178 159 154 157 183 157 181\n",
      " 160 157 172 153 161 155 192 165 180 191 170 167 150 173 173 152 154 154\n",
      " 191 156 199 188 181 179 162 164 173 159 178 150 187 167 168 177 155 184\n",
      " 167 196 193 167 151 169 157 154 157 194 172 194 156 191 194 180 186 186\n",
      " 152 197 156 151 163 180 166 174 166 158 179 169 176 195 177 188 151 169\n",
      " 153 187 191 184 189 181 194 172 171 188 151 164 188 180 151 177 187 197\n",
      " 150 164 167 152 182 186 163 191 155 151 183 197 173 165 187 154 154 172\n",
      " 181 198 194 181 180 192 193 155 159 184 151 193 174 178 198 198 192 159\n",
      " 156 194 186 163 157 157 175 171 157 183 191 187 175 161 168 159 157 174\n",
      " 179 195 177 193 194 194 184 164 173 168 189 172 164 179 160 168 178 182\n",
      " 176 182 177 182 185 172 165 195 189 199 185 154]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "M = 300\n",
    "k = 10\n",
    "N = np.random.randint(150,200,size=M)\n",
    "V = 30\n",
    "\n",
    "print('N: {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "The documents are then generated one by one according to the LDA model (see 2 Algorithm description). Three distinct groups of documents are generated: the first 100 have a strong preference for topics 1, 2, and 3; the second 100 have a strong preference for topics 4, 5, and 6; and the last 100 have a strong preference for topics 7, 8, 9, and 10. Furthermore, each topic will have a strong preference for 3 words, such that each word is prevalent in one topic. The structure of the resulting parameters are shown in Figures 1 and 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3 groups of documents, each with a topic preference\n",
    "alpha_gen1 = np.array((20,15,10,1,1,1,1,1,1,1))\n",
    "alpha_gen2 = np.array((1,1,1,10,15,20,1,1,1,1))\n",
    "alpha_gen3 = np.array((1,1,1,1,1,1,10,12,15,18))\n",
    "\n",
    "# Arbitrarily choose each topic to have 3 very common words\n",
    "beta_probs = np.ones((V,k)) + np.array([np.arange(V)%k==i for i in range(k)]).T*19\n",
    "beta_gen = np.array(list(map(lambda x: np.random.dirichlet(x),beta_probs.T))).T\n",
    "\n",
    "w_struct = list();\n",
    "theta = np.empty((M,k))\n",
    "\n",
    "# Generate each document\n",
    "for m in range(M):\n",
    "    # Draw topic distribution for the document\n",
    "    if m<M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen1,1)[0]\n",
    "    elif m<2*M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen2,1)[0]\n",
    "    else:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen3,1)[0]\n",
    "    doc = np.array([])\n",
    "    \n",
    "    for n in range(N[m]):\n",
    "        # Draw topic according to document's topic distribution\n",
    "        z_n = np.random.choice(np.arange(k),p=theta[m,:])\n",
    "        # Draw word according to topic\n",
    "        w_n = np.random.choice(np.arange(V),p=beta_gen[:,z_n])\n",
    "        doc = np.append(doc,w_n)\n",
    "    w_struct.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_gen](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_gen.png?raw=true)\n",
    "\n",
    "<center>Figure 1: The matrix $\\beta_{gen}$ used to set the topic-word probabilities of the test data. Note how each topic strongly prefers 3 words.</center>\n",
    "\n",
    "![theta_gen](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/theta.png?raw=true)\n",
    "\n",
    "<center>Figure 2: The matrix $\\theta$ generated by the model. Note how there are 3 distinct groups, each with their preferred topics.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters $\\alpha, \\beta, \\phi$ and $\\gamma$\n",
    "The model and variational parameters are then randomly initialized to reasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 100*np.random.dirichlet(10*np.ones(k),1)[0]\n",
    "beta = np.random.dirichlet(np.ones(V),k).T\n",
    "\n",
    "phi = np.array([1/k*np.ones([N[m],k]) for m in range(M)])\n",
    "gamma = np.tile(alpha,(M,1)) + np.tile(N/k,(k,1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Criterion\n",
    "The variational inference parameter $\\gamma$ contains the topic likelihoods of every document. As such, $\\gamma$ identifies to which group a document is likely to belong. As such, the convergence criterion was chosen to monitor this parameter. The root-mean-square of the change in $\\gamma$ is calculated on every iteration of EM and compared against a tolerance parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def converged(gamma,gamma_old,convergence):\n",
    "    print(sqrt(mean(square(gamma-gamma_old))))\n",
    "    return sqrt(mean(square(gamma-gamma_old))) < convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference by iterative EM\n",
    "\n",
    "Expectation-Maximization is carried out by consecutively maximizing each of the four parameters $\\alpha, \\beta, \\phi$ and $\\gamma$ with respect to the log likelihood until either the convergence criterion has been met or a maximimum number of iterations have been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0482680757129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrea/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "convergence = 5*10**(-2.0)\n",
    "successfully_Converged = False\n",
    "max_iters = 10**3\n",
    "\n",
    "for iters in range(max_iters):\n",
    "    #print(iters)\n",
    "    gamma_old = gamma\n",
    "    \n",
    "    ## Expectation step: Update variational parameters\n",
    "    phi   = opt_phi(beta,gamma,w_struct,M,N,k)\n",
    "    gamma = opt_gamma(alpha,phi,M)\n",
    "    \n",
    "    ## Maximization step: Update model parameters\n",
    "    beta  = est_beta(phi,w_struct,k,V)\n",
    "    alpha = est_alpha(alpha,gamma,M,k)\n",
    "    \n",
    "    if converged(gamma,gamma_old,convergence):\n",
    "        successfully_Converged = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After running until the root-mean-square of the difference in $\\gamma$ dropped below 0.05, the algorithm was considered converged and terminated. The results are visualized in plots below in Figures 3 and 4. Since the model parameter $\\beta$ estimated by the algorithm should correspond witht the $\\beta$ used to generate the data, and inferred variational parameter $\\gamma$ should correspond to $\\theta$, Figures 3 and 4 should resemble Figures 1 and 2 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_est_unordered](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_est_unordered.png?raw=true)\n",
    "\n",
    "<center>Figure 3: The matrix $\\beta_{est}$ inferred by the algorithm.</center>\n",
    "\n",
    "![gamma_unordered](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/gamma_unordered.png?raw=true)\n",
    "\n",
    "<center>Figure 4: The matrix $\\gamma$ inferred by the algorithm.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Figures 1 and 2 do not appear to match up with Figures 3 and 4. However, the individual topic identities do not have any specific relation to their index. In other words, there is a non-identifiability issue at play here. It is however apparent in Figure 4 that the algorithm correctly identifies 3 separate groups of documents of the right size. By visually inspecting $\\gamma$ and re-arranging the order of the topics, we get Figures 5 and 6 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_est_rearranged](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_est_rearranged.png?raw=true)\n",
    "\n",
    "<center>Figure 5: The matrix $\\beta_{est}$ inferred by the algorithm, rearranged in original topic order.</center>\n",
    "\n",
    "![gamma_rearranged](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/gamma_rearranged.png?raw=true)\n",
    "\n",
    "<center>Figure 6: The matrix $\\gamma$ inferred by the algorithm, rearranged in original topic order.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect, the visualizations of $\\beta$ and $\\gamma$ with the topics re-arranged now exhibit patterns similar to the original structures visible in Figures 1 and 2. In particular, the three diagonals in $\\beta$ representing the three preferred words of each topic can be clearly seen in Figure 5, and the three boxes corresponding to the preferred topic distributions of the three groups of documents are also apparent in Figure 6. With more data (either more documents or more words in each document), these structures are likely to be recovered with even higher accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
