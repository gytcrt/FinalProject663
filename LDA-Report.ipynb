{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Model with Latent Dirichlet Allocation\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, **Latente Dirichlet Allocation(LDA)** is a widely used topic model, which can automatically discover topics that a corpus contains, and explain similarities between documents. LDA is very intriguing for us, because it is a three-level hierarchical Bayesian model and topic modeling is a classic problem in natural language processing. \n",
    "\n",
    "In the following report,we first describle the mechanism of Latent Dirichlet Allocation. Then we use two methods to implment LDA, one is Variational Inference, and the other is Collapsed Gibbs Sampling. We try to optimize the performance ot our impelmentation by Cython. Additionally We generate test data set based on different topics and visualze the result of topic discovery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Algorithm Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA uses a generative model to explain how the observed words in a corpus which contains many documents are generated from latent variables. The following shows the graphical model representation of LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'LDA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxes are \"plates\" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a documents, and V indicates the vocabulary in the corpus. And we define the following terms:\n",
    "\n",
    "* $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distribution, \n",
    "* $\\beta_i$ is the word distribution for topic k\n",
    "* $\\theta_i$ is the topic distribution for document i,\n",
    "* $z_{mw}$ is the topic of word w in document m\n",
    "\n",
    "LDA assumes the following generative process for each document **m** in a corpus:\n",
    "1. Choose N ~ Poisson($\\xi$).\n",
    "2. Choose $\\theta$ ~ Dir($\\alpha$).\n",
    "3. For each of the N word $w_n$:\n",
    "    1. Choose a topic $z_n$ ~ Multinomial($\\theta$).\n",
    "    2. Choose a word $w_n$ from $p(w_n |z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dirichlet Distribution** is the multivariate generalization of beta distribution, which means the Dirichlet distribution is adistribution over discrete probability distributions. Dirichlet Distributions are oftenly used as conjugate prior distributions of the categorical distribution and multinomial distribution in Bayesian statistics. \n",
    "\n",
    "A *k*-dimensional Dirichlet random variable $\\theta$ can take values in the (k-1)-simplex (a k-vector $\\theta$ lies in the (k-1)-simplex if ${ \\theta  }_{ i }\\ge 0,\\sum _{ i }^{ k }{ { \\alpha  }_{ i } } $), and has the following probability density on the simplex:\n",
    "\n",
    "$$p(\\theta| \\alpha) = \\frac { \\Gamma (\\sum _{ i=1 }^{ k }{ { \\alpha  }_{ i } } ) }{ \\prod _{ i=1 }^{ k }{ \\Gamma ({ \\alpha  }_{ i }) }  } { { \\theta  }_{ 1 } }^{ { \\alpha  }_{ 1 }-1 }\\cdot \\cdot \\cdot { { \\theta  }_{ k } }^{ { \\alpha  }_{ k }-1 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of N topics z, and a set of N words w is given by:\n",
    "\n",
    "$$\n",
    "p(\\theta, z, w|\\alpha, \\beta)=p(\\theta|\\alpha)\\prod _{ n=1 }^{ N }{ p(z_n|\\theta)p(w_n|z_n,\\beta) } \n",
    "$$\n",
    "\n",
    "where $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique i such that { z }_{ n }^{ i }=1. Integrating over $\\theta$ and summing over z, we obtain the marginal distribution of a document:\n",
    "\n",
    "$$\n",
    "p(w|\\alpha, \\beta) = \\int { p(\\theta |\\alpha )(\\prod _{ n=1 }^{ N }{ \\sum { p({ z }_{ n }|\\theta )p({ w }_{ n }|{ z }_{ n },\\beta ) }  } )d\\theta  } \n",
    "$$\n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n",
    "\n",
    "$$\n",
    "p(D|\\alpha, \\beta) = \\prod _{ d=1 }^{ M }{ \\int { p(\\theta_d |\\alpha )(\\prod _{ n=1 }^{ N_d }{ \\sum { p({ z }_{ dn }|\\theta )p({ w }_{ dn }|{ z }_{ dn },\\beta ) }  } )d\\theta_d  }  } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infere the latent parameters is a problem of Bayesian inference. Next, we use Variational Inference and Gibbs Sampling to estimate latent parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Implementation - Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, Blei, Ng and Jordan (2002) gave a variational inference approximation of the posterior distribution, because posterior distribution is usually intractable. But since then, Gibbs Sampling has also become a commonly used way to infere latent parameters in LDA. Here we use Gibbs Sampling to implement LDA. Gibbs Sampling is a Markov Chain Monte Carol methods, of which the next state is reached by sequentially sampling from full conditional distribution of all other variables and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Dirichlet distribution is conjugate prior of the multinomial distribution, the posteriors of $\\theta_i$ and $\\beta_i$ also follow Dirichlet distribution. Their posterior means are:\n",
    "\n",
    "$$\n",
    "\\theta_{i,k} = \\frac { { n }_{ i }^{ k }+{ \\alpha  }_{ k } }{ \\sum _{ k=1 }^{ K }{ { n }_{ i }^{ k }+{ \\alpha  }_{ k } }  } \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_{k,w} = \\frac { { n }_{ w }^{ k }+{ \\beta  }_{ w } }{ \\sum _{ w=1 }^{ W }{ { n }_{ w }^{ k }+{ \\beta  }_{ w } }  } \n",
    "$$\n",
    "\n",
    "where ${ n }_{ i }^{ k }$ is the number of words in document i that have been assigned to topic k, ${ n }_{ w }^{ k }$ is the total number word w assigned to topic k among all documents in the corpus.\n",
    "\n",
    "Obviously, the inference of $\\theta$ and $\\beta$ only depends on assignments of each word to topics $z_i$. Therefore, we can only focus on estimation of $z_i$. And here we define some terms:\n",
    "\n",
    "* $n_m$: the word count of document m, not including the current one \n",
    "* $n_{mz}$: the number of words from document m assigned to topic z, not including the current one\n",
    "* $n_{zw}$: the number of instances of word w assigned to topic z, not including the current one\n",
    "* $n_z$: the total number of words assigned to topic z, not including the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the posterior distribution of word assignment is:\n",
    "\n",
    "$$\n",
    "p(z_i=j|z_i,w)\\propto \\frac { n_{zw} + \\phi }{ n_z + V\\phi } \\cdot \\frac { n_{mz} + \\alpha }{ n_m + K\\alpha } \n",
    "$$\n",
    "\n",
    "And we can implement LDA by Gibbs Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic asigned to word:       $z = 1,...,K$\n",
    "\n",
    "word:        $w = 1,...,N_V$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "Z: topic assigned to word w\n",
    "\n",
    "$\\theta: K \\times N$ \n",
    "\n",
    "$\\beta: M \\times K$ \n",
    "\n",
    "$Multinomial(\\theta)$: distribution over words for a given topic\n",
    "\n",
    "$Multinomial(\\beta)$: distribution over topics for a given document\n",
    "\n",
    "$n_m$, $n_{mz}$, $n_{zw}$, $n_z$: as defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words count function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_count_doc(corpus):\n",
    "    \"\"\"\n",
    "    Count the toal number of words in each document in corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : a list-like, contains bag-of-words of each document\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_m : a np.array, shape(M)\n",
    "         the total number of words in each document\n",
    "    \"\"\"\n",
    "    n_m = []\n",
    "    for i in range(len(corpus)):\n",
    "        n_m.append(np.sum(corpus[i], axis = 0)[1])\n",
    "    return np.array(n_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize empty parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_parameters(corpus, K, V):\n",
    "    \"\"\"\n",
    "    Initialize empty parameter n_mz, n_zw, n_z.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    K : int, the number of topics\n",
    "    V : int, the number of vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    z_mw : the topic of word w in document m\n",
    "    n_mz : the number of words from document m assigned to topic z\n",
    "    n_zw : the number of words assigned topic z\n",
    "    n_z : the total number of words assigned to topic z\n",
    "    \"\"\"\n",
    "    z_mw = []\n",
    "    n_mz = np.zeros((len(corpus), K))\n",
    "    n_zw = np.zeros((K, V))\n",
    "    n_z = np.zeros(K)\n",
    "    return z_mw, n_mz, n_zw, n_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters based on words in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_parameters(corpus, K, V):\n",
    "    \"\"\"\n",
    "    Initialize parameters for the corpus \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    corpus: a list-like, contains bag-of-words of each document\n",
    "    K : int, the number of topics\n",
    "    V : int, the number of vocabulary\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    z_mw : the topic of word w in document m\n",
    "    n_mz : the number of words from document m assigned to topic z\n",
    "    n_zw : the number of words assigned topic z\n",
    "    n_z : the total number of words assigned to topic z\n",
    "    \n",
    "    \"\"\"\n",
    "    z_mw, n_mz, n_zw, n_z = empty_parameters(corpus, K, V)\n",
    "    z_mw = []\n",
    "    for m, doc in enumerate(corpus):\n",
    "        z_n = []\n",
    "        for n, t in doc:\n",
    "            z = np.random.randint(0, K)\n",
    "            z_n.append(z)\n",
    "            n_mz[m, z] += t\n",
    "            n_zw[z, n] += t\n",
    "            n_z[z] += t\n",
    "        z_mw.append(np.array(z_n))\n",
    "    return z_mw, n_mz, n_zw, n_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sampling(corpus, max_iter, K, V, n_zw, n_z, n_mz, n_m, z_mw, alpha, phi):\n",
    "    beta_gibbs = []\n",
    "    theta_gibbs = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        for m, doc in enumerate(corpus):\n",
    "            for n, (w, t) in enumerate(doc):\n",
    "                #exclude the current word\n",
    "                z = z_mw[m][n]\n",
    "                n_mz[m, z] -= t\n",
    "                n_m[m] -= t\n",
    "                n_zw[z, w] -= t\n",
    "                n_z[z] -= t\n",
    "        \n",
    "                new_z = sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m)\n",
    "\n",
    "                #include the current word\n",
    "                z_mw[m][n] = new_z\n",
    "                n_mz[m, new_z] += t\n",
    "                n_zw[new_z, w] += t\n",
    "                n_z[new_z] += t\n",
    "                n_m[m] += t\n",
    "\n",
    "        #update beta\n",
    "        beta_gibbs.append(update_beta(V, n_zw, n_z, alpha))\n",
    "        #update theta\n",
    "        theta_gibbs.append(update_theta(K, n_mz, n_m, phi))\n",
    "    return beta_gibbs, theta_gibbs\n",
    "\n",
    "def sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m):\n",
    "    \"\"\"\n",
    "    Sample new topic for current word\n",
    "    \n",
    "    \"\"\"\n",
    "    p_z = np.zeros(K)\n",
    "    for j in range(K):\n",
    "        p_z[j] = ((n_zw[j, w] + phi)/(n_z[j] + V * phi)) * ((n_mz[m, j] + alpha)/(n_m[m] + K * alpha))\n",
    "    new_z = np.random.multinomial(1, p_z/p_z.sum()).argmax()\n",
    "    return new_z    \n",
    "\n",
    "def update_beta(V, n_zw, n_z, alpha):\n",
    "    \"\"\"\n",
    "    Update beta\n",
    "    \"\"\"\n",
    "    beta = (n_zw + alpha)/(n_z[:,None] + V *alpha)\n",
    "    return beta\n",
    "\n",
    "def update_theta(K, n_mz, n_m, phi):\n",
    "    \"\"\"\n",
    "    Update theta\n",
    "    \"\"\"\n",
    "    theta = (n_mz + phi)/(n_m[:, None] + K * phi)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
