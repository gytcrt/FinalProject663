{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Model with Latent Dirichlet Allocation\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, **Latente Dirichlet Allocation(LDA)** is a widely used topic model, which can automatically discover topics that a corpus contains, and explain similarities between documents. LDA is very intriguing for us, because it is a three-level hierarchical Bayesian model and topic modeling is a classic problem in natural language processing. \n",
    "\n",
    "In the following report,we first describle the mechanism of Latent Dirichlet Allocation. Then we use two methods to implment LDA, one is Variational Inference, and the other is Collapsed Gibbs Sampling. We try to optimize the performance ot our impelmentation by Cython. Additionally We generate test data set based on different topics and visualze the result of topic discovery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA uses a generative model to explain how the observed words in a corpus which contains many documents are generated from latent variables. The following shows the graphical model representation of LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'LDA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxes are \"plates\" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a documents. And we define the following terms:\n",
    "\n",
    "* $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distribution, \n",
    "* $\\beta_i$ is the word distribution for topic k\n",
    "* $\\theta_i$ is the topic distribution for document i,\n",
    "* $z_{mw}$ is the topic of word w in document m\n",
    "\n",
    "LDA assumes the following generative process for each document **m** in a corpus:\n",
    "1. Choose N ~ Poisson($\\xi$).\n",
    "2. Choose $\\theta$ ~ Dir($\\alpha$).\n",
    "3. For each of the N word $w_n$:\n",
    "    1. Choose a topic $z_n$ ~ Multinomial($\\theta$).\n",
    "    2. Choose a word $w_n$ from $p(w_n |z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dirichlet Distribution** is the multivariate generalization of beta distribution, which means the Dirichlet distribution is adistribution over discrete probability distributions. Dirichlet Distributions are oftenly used as conjugate prior distributions of the categorical distribution and multinomial distribution in Bayesian statistics. \n",
    "\n",
    "A *k*-dimensional Dirichlet random variable $\\theta$ can take values in the (k-1)-simplex (a k-vector $\\theta$ lies in the (k-1)-simplex if ${ \\theta  }_{ i }\\ge 0,\\sum _{ i }^{ k }{ { \\alpha  }_{ i } } $), and has the following probability density on the simplex:\n",
    "\n",
    "$$p(\\theta| \\alpha) = \\frac { \\Gamma (\\sum _{ i=1 }^{ k }{ { \\alpha  }_{ i } } ) }{ \\prod _{ i=1 }^{ k }{ \\Gamma ({ \\alpha  }_{ i }) }  } { { \\theta  }_{ 1 } }^{ { \\alpha  }_{ 1 }-1 }\\cdot \\cdot \\cdot { { \\theta  }_{ k } }^{ { \\alpha  }_{ k }-1 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of N topics z, and a set of N words w is given by:\n",
    "\n",
    "$$\n",
    "p(\\theta, z, w|\\alpha, \\beta)=p(\\theta|\\alpha)\\prod _{ n=1 }^{ N }{ p(z_n|\\theta)p(w_n|z_n,\\beta) } \n",
    "$$\n",
    "\n",
    "where $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique i such that { z }_{ n }^{ i }=1. Integrating over $\\theta$ and summing over z, we obtain the marginal distribution of a document:\n",
    "\n",
    "$$\n",
    "p(w|\\alpha, \\beta) = \\int { p(\\theta |\\alpha )(\\prod _{ n=1 }^{ N }{ \\sum { p({ z }_{ n }|\\theta )p({ w }_{ n }|{ z }_{ n },\\beta ) }  } )d\\theta  } \n",
    "$$\n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n",
    "\n",
    "$$\n",
    "p(D|\\alpha, \\beta) = \\prod _{ d=1 }^{ M }{ \\int { p(\\theta_d |\\alpha )(\\prod _{ n=1 }^{ N_d }{ \\sum { p({ z }_{ dn }|\\theta )p({ w }_{ dn }|{ z }_{ dn },\\beta ) }  } )d\\theta_d  }  } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infere the latent parameters is a problem of Bayesian inference. Next, we use Variational Inference and Gibbs Sampling to estimate latent parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
