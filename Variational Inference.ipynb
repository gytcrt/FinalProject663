{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation - Variational Inference\n",
    "====\n",
    "\n",
    "Based on the paper \"Latent Dirchlet Allocation\" by David M. Blei, Andrew Y. Ng, Michael I. Jordan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from scipy.special import digamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic:       $z = 1,...,k$\n",
    "\n",
    "word:        $w = 1,...,N_m$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "$\\alpha: 1 \\times k$ vector of topic distribution probabilities\n",
    "\n",
    "$\\beta: k \\times v$ matrix of word probabilities for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test data and pre-processing\n",
    "\n",
    "Run the following first:\n",
    "```\n",
    "pip install -U nltk\n",
    "pip install stop-words\n",
    "pip install -U gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eb0e6d5a83e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# loop through document list\n",
    "for doc in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: [23 28 40]\n"
     ]
    }
   ],
   "source": [
    "M = 3\n",
    "k = 10\n",
    "N = np.random.randint(50,size=M)\n",
    "V = 20\n",
    "\n",
    "print('N: {0}'.format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([ 7,  7, 18, 18,  8,  9,  6,  1,  6, 18,  2,  9,  8, 11,  1, 19, 14,\n",
       "       17, 15, 19,  3,  8,  8]),\n",
       "       array([ 7,  9,  4,  1, 12,  6,  4, 18, 14, 18,  3,  9,  2, 10,  4,  8, 14,\n",
       "        6,  3,  8, 15,  7,  4,  3, 18,  9, 17, 13]),\n",
       "       array([18, 12, 16, 13, 19, 18,  1, 13,  0, 17,  9, 13,  2, 16, 16,  7,  6,\n",
       "       12, 19,  0, 16, 18,  5,  9, 16, 18, 18, 19,  6, 10, 13, 10, 19, 13,\n",
       "       16,  3,  9, 12, 13, 15])], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random \"documents\"\n",
    "doc1 = np.random.randint(V,size=N[0])\n",
    "doc2 = np.random.randint(V,size=N[1])\n",
    "doc3 = np.random.randint(V,size=N[2])\n",
    "\n",
    "w = np.array((doc1,doc2,doc3))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word #11 in document 2 (w_dn)\n",
    "w[1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False], dtype=bool),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False,  True, False, False, False, False], dtype=bool),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False], dtype=bool)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc == 3 for doc in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0]),\n",
       "       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0]),\n",
       "       array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0])], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc == 3 for doc in w]*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters $\\alpha, \\beta, \\phi$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = np.random.dirichlet(np.ones(k),1)\n",
    "beta = np.random.dirichlet(np.ones(k),V)\n",
    "\n",
    "phi = np.array([1/k*np.ones([N[m],k]) for m in range(M)])\n",
    "gamma = np.tile(alpha,(M,1)) + np.tile(N/k,(k,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03614628,  0.0682012 ,  0.04293727,  0.08103403,  0.03368725,\n",
       "         0.41405727,  0.14638083,  0.01358403,  0.05414736,  0.10982448]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize variational parameters $\\phi$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split phi and gamma optimization apart for parallelization purposes\n",
    "# TODO: See if some sort of vectorization is possible for speed-up\n",
    "def optVarParams(alpha,beta,phi,gamma,words):\n",
    "    # Optimize phi\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(k):\n",
    "                phi[m][n,i] = beta[words[m][n],i] * np.exp(digamma(gamma[m,i]) - digamma(np.sum(gamma[m,:])))\n",
    "    \n",
    "    # Optimize gamma\n",
    "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi))).T\n",
    "    \n",
    "    return phi,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,10) (10,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a37204028e5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptVarParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-26789e5b2de7>\u001b[0m in \u001b[0;36moptVarParams\u001b[1;34m(alpha, beta, phi, gamma, words)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Optimize gamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,10) (10,3) "
     ]
    }
   ],
   "source": [
    "optVarParams(alpha,beta,phi,gamma,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estModParams(alpha,beta,phi,gamma,words):\n",
    "    # Optimize beta\n",
    "    for i in range(k):\n",
    "        for j in range (V):\n",
    "            w_dnj = [doc == j for doc in w]\n",
    "            beta[i,j] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi*w_dnj))),axis=0)\n",
    "\n",
    "\n",
    "    return alpha,beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convergence = 10**(-3)\n",
    "\n",
    "'''\n",
    "Pseudocode\n",
    "\n",
    "while(!converged):\n",
    "    phi,gamma  = optVarParams(alpha,beta,phi,gamma)\n",
    "    alpha,beta = estModParams(alpha,beta,phi,gamma)\n",
    "    if converged(alpha,beta):\n",
    "        break\n",
    "'''        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests \n",
    "Testing out syntax and array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi))),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
