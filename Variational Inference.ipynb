{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation - Variational Inference\n",
    "====\n",
    "\n",
    "Based on the paper \"Latent Dirchlet Allocation\" by David M. Blei, Andrew Y. Ng, Michael I. Jordan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from scipy.special import digamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic:       $z = 1,...,k$\n",
    "\n",
    "word:        $w = 1,...,N_m$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "$\\alpha: 1 \\times k$ vector of topic distribution probabilities\n",
    "\n",
    "$\\beta: k \\times v$ matrix of word probabilities for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test data and pre-processing\n",
    "\n",
    "Run the following first:\n",
    "```\n",
    "pip install -U nltk\n",
    "pip install stop-words\n",
    "pip install -U gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /opt/conda/lib/python3.4/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): stop-words in /opt/conda/lib/python3.4/site-packages\n",
      "Collecting gensim\n",
      "  Using cached gensim-0.12.4.tar.gz\n",
      "Collecting numpy>=1.3 (from gensim)\n",
      "  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: scipy>=0.7.0 in /opt/conda/lib/python3.4/site-packages (from gensim)\n",
      "Requirement already up-to-date: six>=1.5.0 in /opt/conda/lib/python3.4/site-packages/six-1.10.0-py3.4.egg (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Using cached smart_open-1.3.2.tar.gz\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Using cached boto-2.39.0-py2.py3-none-any.whl\n",
      "Collecting httpretty==0.8.10 (from smart-open>=1.2.1->gensim)\n",
      "  Using cached httpretty-0.8.10.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-em60bd2t/httpretty/setup.py\", line 86, in <module>\n",
      "        version=read_version(),\n",
      "      File \"/tmp/pip-build-em60bd2t/httpretty/setup.py\", line 46, in read_version\n",
      "        finder.visit(ast.parse(local_file('httpretty', '__init__.py')))\n",
      "      File \"/tmp/pip-build-em60bd2t/httpretty/setup.py\", line 78, in <lambda>\n",
      "        open(os.path.join(os.path.dirname(__file__), *f)).read()\n",
      "      File \"/opt/conda/lib/python3.4/encodings/ascii.py\", line 26, in decode\n",
      "        return codecs.ascii_decode(input, self.errors)[0]\n",
      "    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 133: ordinal not in range(128)\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-em60bd2t/httpretty/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk\n",
    "!pip install stop-words\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eb0e6d5a83e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: [23 28 40]\n"
     ]
    }
   ],
   "source": [
    "M = 3\n",
    "k = 10\n",
    "N = np.random.randint(50,size=M)\n",
    "V = 100\n",
    "\n",
    "print('N: {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters $\\alpha, \\beta, \\phi$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = np.random.dirichlet(np.ones(k),1)\n",
    "beta = np.random.dirichlet(np.ones(k),V)\n",
    "\n",
    "phi = np.array([1/k*np.ones([N[m],k]) for m in range(M)])\n",
    "gamma = np.tile(alpha,(M,1)) + np.tile(N/k,(k,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03614628,  0.0682012 ,  0.04293727,  0.08103403,  0.03368725,\n",
       "         0.41405727,  0.14638083,  0.01358403,  0.05414736,  0.10982448]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize variational parameters $\\phi$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split phi and gamma optimization apart for parallelization purposes\n",
    "# TODO: See if some sort of vectorization is possible for speed-up\n",
    "def optVarParams(alpha,beta,phi,gamma,words):\n",
    "    # Optimize phi\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(k):\n",
    "                phi[m][n,i] = beta[i,words[m][n]] * np.exp(digamma(gamma[m,i]) - digamma(np.sum(gamma[m,:])))\n",
    "    \n",
    "    # Optimize gamma\n",
    "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi))).T\n",
    "    \n",
    "    return phi,gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estModParams(alpha,beta,phi,gamma,words):\n",
    "    # Optimize beta\n",
    "    for i in range(k):\n",
    "        for j in range (V):\n",
    "            beta[i,j] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi)),axis=0)\n",
    "\n",
    "\n",
    "    return alpha,beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convergence = 10**(-3)\n",
    "\n",
    "'''\n",
    "Pseudocode\n",
    "\n",
    "while(!converged):\n",
    "    phi,gamma  = optVarParams(alpha,beta,phi,gamma)\n",
    "    alpha,beta = estModParams(alpha,beta,phi,gamma)\n",
    "    if converged(alpha,beta):\n",
    "        break\n",
    "'''        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests \n",
    "Testing out syntax and array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1,  9.1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi))),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
