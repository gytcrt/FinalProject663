{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling with Latent Dirichlet Allocation\n",
    "==\n",
    "Paper: Latent Dirichlet Allocation\n",
    "\n",
    "Authors: David M Blei, Andrew Y Ng, Michael I Jordan \n",
    "\n",
    "STA663:\n",
    "Yuan Gao,\n",
    "Kevin Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, **Latent Dirichlet Allocation (LDA)** is a widely used topic model proposed by David Blei, Andrew Ng, and Michael Jordan, capable of automatically discovering topics that documents in a corpus contain and explaining similarities between documents. LDA is very intriguing for us, because it is a three-level hierarchical Bayesian model, and topic modeling is a classic problem in natural language processing. \n",
    "\n",
    "In the following report, we first describe the mechanism of Latent Dirichlet Allocation. We then use two methods to implement LDA: Variational Inference and Collapsed Gibbs Sampling. Next, we try to optimize the performance of our implementation with Cython. Finally, we generate a test data set based on different topics and visualize the result of topic discovery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA uses a generative model to explain how the observed words in the documents of a corpus are generated from latent variables. The following shows the graphical model representation of LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Output_Data/LDA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxes are \"plates\" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of document, N the number of words in a documents, and V indicates the size of the vocabulary of the corpus. We define the following terms:\n",
    "\n",
    "* $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distribution, \n",
    "* $\\beta_i$ is the word distribution for topic k\n",
    "* $\\theta_m$ is the topic distribution for document m,\n",
    "* $z_{mw}$ is the topic of word w in document m\n",
    "\n",
    "LDA assumes the following generative process for each document **m** in a corpus:\n",
    "1. Choose $N$ ~ Poisson($\\xi$).\n",
    "2. Choose $\\theta$ ~ Dir($\\alpha$).\n",
    "3. For each of the $N$ word $w_n$:\n",
    "    1. Choose a topic $z_n$ ~ Multinomial($\\theta$).\n",
    "    2. Choose a word $w_n$ from $p(w_n |z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n",
    "    \n",
    "Note that the length of each document $N$ does not interact with any of the other variables and is thus just assumed to be known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dirichlet Distribution** is the multivariate generalization of the beta distribution, which means the Dirichlet distribution is a distribution over discrete probability distributions. Dirichlet distributions are oftenly used as conjugate prior distributions of the categorical distribution and multinomial distribution in Bayesian statistics. \n",
    "\n",
    "A *k*-dimensional Dirichlet random variable $\\theta$ can take values in the (k-1)-simplex (a k-vector $\\theta$ lies in the (k-1)-simplex if ${ \\theta  }_{ i }\\ge 0,\\sum _{ i }^{ k }{ { \\alpha  }_{ i } } $), and has the following probability density on the simplex:\n",
    "\n",
    "$$p(\\theta| \\alpha) = \\frac { \\Gamma (\\sum _{ i=1 }^{ k }{ { \\alpha  }_{ i } } ) }{ \\prod _{ i=1 }^{ k }{ \\Gamma ({ \\alpha  }_{ i }) }  } { { \\theta  }_{ 1 } }^{ { \\alpha  }_{ 1 }-1 }\\cdot \\cdot \\cdot { { \\theta  }_{ k } }^{ { \\alpha  }_{ k }-1 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $z$, and a set of $N$ words $w$ is given by:\n",
    "\n",
    "$$\n",
    "p(\\theta, z, w|\\alpha, \\beta)=p(\\theta|\\alpha)\\prod _{ n=1 }^{ N }{ p(z_n|\\theta)p(w_n|z_n,\\beta) } \n",
    "$$\n",
    "\n",
    "where $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique i such that ${ z }_{ n }^{ i }=1$. Integrating over $\\theta$ and summing over z, we obtain the marginal distribution of a document:\n",
    "\n",
    "$$\n",
    "p(w|\\alpha, \\beta) = \\int { p(\\theta |\\alpha )(\\prod _{ n=1 }^{ N }{ \\sum { p({ z }_{ n }|\\theta )p({ w }_{ n }|{ z }_{ n },\\beta ) }  } )d\\theta  } \n",
    "$$\n",
    "\n",
    "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n",
    "\n",
    "$$\n",
    "p(D|\\alpha, \\beta) = \\prod _{ d=1 }^{ M }{ \\int { p(\\theta_d |\\alpha )(\\prod _{ n=1 }^{ N_d }{ \\sum { p({ z }_{ dn }|\\theta )p({ w }_{ dn }|{ z }_{ dn },\\beta ) }  } )d\\theta_d  }  } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the latent parameters is a problem of Bayesian inference. Next, we use Variational Inference and Gibbs Sampling to estimate latent parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Implementation - Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, Blei, Ng and Jordan (2002) gave a variational inference approximation of the posterior distribution, because posterior distribution is usually intractable. Since then though, Gibbs Sampling has also become a commonly used way to infer latent parameters in LDA. Here, we use Gibbs Sampling to implement LDA. Gibbs Sampling is a Markov Chain Monte Carlo method, in which the next state is reached by sequentially sampling from the full conditional distributions of all other variables and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Dirichlet distribution is conjugate prior of the multinomial distribution, the posteriors of $\\theta_i$ and $\\beta_i$ also follow the Dirichlet distribution. Their posterior means are:\n",
    "\n",
    "$$\n",
    "\\theta_{i,k} = \\frac { { n }_{ i }^{ k }+{ \\alpha  }_{ k } }{ \\sum _{ k=1 }^{ K }{ { n }_{ i }^{ k }+{ \\alpha  }_{ k } }  } \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_{k,w} = \\frac { { n }_{ w }^{ k }+{ \\beta  }_{ w } }{ \\sum _{ w=1 }^{ W }{ { n }_{ w }^{ k }+{ \\beta  }_{ w } }  } \n",
    "$$\n",
    "\n",
    "where ${ n }_{ i }^{ k }$ is the number of words in document i that have been assigned to topic k, ${ n }_{ w }^{ k }$ is the total number words $w$ assigned to topic $k$ among all documents in the corpus.\n",
    "\n",
    "Obviously, the inference of $\\theta$ and $\\beta$ only depends on assignments of each word to topics $z_i$. Therefore, we can only focus on estimation of $z_i$. We define some terms:\n",
    "\n",
    "* $n_m$: the word count of document $m$, not including the current one \n",
    "* $n_{mz}$: the number of words from document $m$ assigned to topic $z$, not including the current one\n",
    "* $n_{zw}$: the number of instances of word $w$ assigned to topic $z$, not including the current one\n",
    "* $n_z$: the total number of words assigned to topic $z$, not including the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the posterior distribution of word assignment is:\n",
    "\n",
    "$$\n",
    "p(z_i=j|z_i,w)\\propto \\frac { n_{zw} + \\phi }{ n_z + V\\phi } \\cdot \\frac { n_{mz} + \\alpha }{ n_m + K\\alpha } \n",
    "$$\n",
    "\n",
    "And we can implement LDA by Gibbs Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic asigned to word:       $z = 1,...,K$\n",
    "\n",
    "word:        $w = 1,...,N_V$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "Z: topic assigned to word w\n",
    "\n",
    "$\\theta: K \\times N$ \n",
    "\n",
    "$\\beta: M \\times K$ \n",
    "\n",
    "$Multinomial(\\theta)$: distribution over words for a given topic\n",
    "\n",
    "$Multinomial(\\beta)$: distribution over topics for a given document\n",
    "\n",
    "$n_m$, $n_{mz}$, $n_{zw}$, $n_z$: as defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt,mean,square\n",
    "from scipy.special import digamma, polygamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_count_doc(corpus):\n",
    "    \"\"\"\n",
    "    Count the total number of words in each document in corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : a list-like structure, contains bag-of-words of each document\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_m : a np.array, shape(M)\n",
    "         the total number of words in each document\n",
    "    \"\"\"\n",
    "    n_m = []\n",
    "    for i in range(len(corpus)):\n",
    "        n_m.append(np.sum(corpus[i], axis = 0)[1])\n",
    "    return np.array(n_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize empty parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_parameters(corpus, K, V):\n",
    "    \"\"\"\n",
    "    Initialize empty parameters n_mz, n_zw, n_z.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    K : int, the number of topics\n",
    "    V : int, the number of vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    z_mw : the topic of word w in document m\n",
    "    n_mz : the number of words from document m assigned to topic z\n",
    "    n_zw : the number of words assigned topic z\n",
    "    n_z : the total number of words assigned to topic z\n",
    "    \"\"\"\n",
    "    z_mw = []\n",
    "    n_mz = np.zeros((len(corpus), K))\n",
    "    n_zw = np.zeros((K, V))\n",
    "    n_z = np.zeros(K)\n",
    "    return z_mw, n_mz, n_zw, n_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters based on words in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_parameters(corpus, K, V):\n",
    "    \"\"\"\n",
    "    Initialize parameters for the corpus \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    corpus: a list-like structure, contains bag-of-words of each document\n",
    "    K : int, the number of topics\n",
    "    V : int, the size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    z_mw : the topic of word w in document m\n",
    "    n_mz : the number of words from document m assigned to topic z\n",
    "    n_zw : the number of words assigned topic z\n",
    "    n_z : the total number of words assigned to topic z\n",
    "    \n",
    "    \"\"\"\n",
    "    z_mw, n_mz, n_zw, n_z = empty_parameters(corpus, K, V)\n",
    "    z_mw = []\n",
    "    for m, doc in enumerate(corpus):\n",
    "        z_n = []\n",
    "        for n, t in doc:\n",
    "            z = np.random.randint(0, K)\n",
    "            z_n.append(z)\n",
    "            n_mz[m, z] += t\n",
    "            n_zw[z, n] += t\n",
    "            n_z[z] += t\n",
    "        z_mw.append(np.array(z_n))\n",
    "    return z_mw, n_mz, n_zw, n_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sampling(corpus, max_iter, K, V, n_zw, n_z, n_mz, n_m, z_mw, alpha, phi):\n",
    "    beta_gibbs = []\n",
    "    theta_gibbs = []\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    for i in range(max_iter):\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        for m, doc in enumerate(corpus):\n",
    "            for n, (w, t) in enumerate(doc):\n",
    "                #exclude the current word\n",
    "                z = z_mw[m][n]\n",
    "                n_mz[m, z] -= t\n",
    "                n_m[m] -= t\n",
    "                n_zw[z, w] -= t\n",
    "                n_z[z] -= t\n",
    "        \n",
    "                new_z = sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m)\n",
    "\n",
    "                #include the current word\n",
    "                z_mw[m][n] = new_z\n",
    "                n_mz[m, new_z] += t\n",
    "                n_zw[new_z, w] += t\n",
    "                n_z[new_z] += t\n",
    "                n_m[m] += t\n",
    "\n",
    "        #update beta\n",
    "        beta_gibbs.append(update_beta(V, n_zw, n_z, alpha))\n",
    "        #update theta\n",
    "        theta_gibbs.append(update_theta(K, n_mz, n_m, phi))\n",
    "    return beta_gibbs, theta_gibbs\n",
    "\n",
    "def sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m):\n",
    "    \"\"\"\n",
    "    Sample new topic for current word\n",
    "    \n",
    "    \"\"\"\n",
    "    p_z = np.zeros(K)\n",
    "    for j in range(K):\n",
    "        p_z[j] = ((n_zw[j, w] + phi)/(n_z[j] + V * phi)) * ((n_mz[m, j] + alpha)/(n_m[m] + K * alpha))\n",
    "    new_z = np.random.multinomial(1, p_z/p_z.sum()).argmax()\n",
    "    return new_z    \n",
    "\n",
    "def update_beta(V, n_zw, n_z, alpha):\n",
    "    \"\"\"\n",
    "    Update beta\n",
    "    \"\"\"\n",
    "    beta = (n_zw + alpha)/(n_z[:,None] + V *alpha)\n",
    "    return beta\n",
    "\n",
    "def update_theta(K, n_mz, n_m, phi):\n",
    "    \"\"\"\n",
    "    Update theta\n",
    "    \"\"\"\n",
    "    theta = (n_mz + phi)/(n_m[:, None] + K * phi)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implementation - Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation was also implemented using variational inference. In situations where variational inference is typically used, the posterior is typically intractable to calculate directly. In the case of LDA, the posterior $p(\\theta,z,w | \\alpha,\\beta)$ is difficult to compute, so the distribution is instead approximated with the variational distribution:\n",
    "\n",
    "$$q(\\theta,z | \\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^{N} q(z_n|\\phi_n)$$\n",
    "\n",
    "Using Jensen's inequality, it can be shown that the difference between the log likelihood of the true posterior and the variational approximation is the KL-divergence between the two. In order words:\n",
    "\n",
    "$$ \\log(p(w|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,z|\\gamma,\\phi)||p(\\theta,z|w,\\alpha,\\beta))$$\n",
    "\n",
    "We can choose to either minimize the KL-divergence or maximize the likelihood. Here, the latter is approach is taken. Factoring the likelihood appropriately, we can write the following:\n",
    "\n",
    "$$ L(\\gamma,\\phi;\\alpha,\\beta) = E_q[\\log p(\\theta|\\alpha)] + E_q[\\log p(z|\\theta)] + E_q [\\log p(w|z,\\beta)] - E_q [\\log q(\\theta)] - E_q[\\log q(z)] $$\n",
    "\n",
    "This likelihood is maximized through Expectation-Maximization (EM). During the expectation step, the variational parameters $\\phi$ and $\\gamma$ are first optimized by maximizing the likelihood with respect to each individually. During the maximization step, the likelihood is then maximized with respect to model parameters $\\alpha$ and $\\beta$. This process is outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Parameters\n",
    "\n",
    "document:    $m = 1,...,M$\n",
    "\n",
    "topic:       $z = 1,...,k$\n",
    "\n",
    "word:        $w = 1,...,N_m$\n",
    "\n",
    "vocabulary : $v = 1,...,V$\n",
    "\n",
    "$\\alpha: 1 \\times k$ Model parameter - vector of topic distribution probabilities for each document\n",
    "\n",
    "$\\beta: k \\times v$ Model parameter - matrix of word probabilities for each topic\n",
    "\n",
    "$\\phi: M \\times N_m \\times k$ Variational parameter - matrix of topic probabilities for each word in each document\n",
    "\n",
    "$\\gamma: M \\times k$ Variational parameter - matrix of topic probabilities for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize variational parameters $\\phi$ and $\\gamma$\n",
    "\n",
    "By taking the derivative the log likelihood with respect to $\\phi$ and setting the result to zero, we find the maximal value of $\\phi$:\n",
    "\n",
    "$$ \\phi_{ni} \\propto \\beta_{iv} \\exp(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k(\\gamma_j)) $$\n",
    "\n",
    "where $\\beta_{iv}$ = $p(w_n^v = 1|z_n = i)$ and $\\Psi$ is the digamma function (derivative of the log gamma function $\\Gamma$). As $\\phi$ represents the probability of each word in a document for each latent topic, these values must be normalized such that each row representing a word position within a document must sum to 1.\n",
    "\n",
    "\n",
    "In a similar fashion, it can be shown that $\\gamma$ is maximized at:\n",
    "\n",
    "$$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N(\\phi_{ni})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize variational parameter phi\n",
    "def opt_phi(beta,gamma,words,M,N,k):\n",
    "    for m in range(M):\n",
    "        for n in range(N[m]):\n",
    "            for i in range(k):\n",
    "                phi[m][n,i] = beta[words[m][n],i] * np.exp(digamma(gamma[m,i]) - digamma(np.sum(gamma[m,:])))\n",
    "            # Normalize across states so phi represents probability over states for each word\n",
    "            phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
    "    return phi\n",
    "\n",
    "\n",
    "## Optimize variational parameter gamma\n",
    "def opt_gamma(alpha,phi,M):\n",
    "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi)))\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters $\\alpha$ and $\\beta$\n",
    "\n",
    "By taking the derivative of the log likelihood and applying the appropriate Lagrange multipliers to ensure probabilities sum to 1, we find that $/beta$ is maximized with:\n",
    "\n",
    "$$ \\beta_{ij} \\propto \\sum_{m=1}^M \\sum_{n=1}^{N_m} \\phi_{dni}w_{mn}^j$$\n",
    "\n",
    "where $w_{mn}^j$ = 1 if the $n^{th}$ word of document $m$ is equal to $j$, and 0 otherwise. Since the columns of \\beta represent the probability of each word given the topic of that particular column, they must be normalized to sum to 1.\n",
    "\n",
    "Taking the derivative of the log likelihood with respect to $\\alpha$ yields:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial\\alpha_i} = M(\\Psi(\\sum_{j=1}^k\\alpha_j)-\\Psi(\\alpha_i)) - \\sum_{m=1}^M(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}))$$\n",
    "\n",
    "Because this is difficult to find the zero intercept of this derivative, $\\alpha$ is instead maximized numerically with the Newton-Raphson method. The Hessian is of the form:\n",
    "\n",
    "$$ \\frac{\\partial^2 L}{\\partial\\alpha_i\\partial\\alpha_j} = M(\\Psi'(\\sum_{j=1}^k \\alpha_j) - \\delta(i,j)\\Psi'(\\alpha_i))$$ \n",
    "\n",
    "Note: This is slightly different from what is stated in the paper, which has a couple errors in the reported form of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimize beta\n",
    "def est_beta(phi,words,k,V):\n",
    "    for j in range (V):\n",
    "        # Construct w_mn == j of same shape as phi\n",
    "        w_mnj = [np.tile((word == j),(k,1)).T for word in words]\n",
    "        beta[j,:] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))),axis=0)\n",
    "        \n",
    "    # Normalize across states so beta represents probability of each word given the state\n",
    "    for i in range(k):\n",
    "        beta[:,i] = beta[:,i]/sum(beta[:,i])\n",
    "        \n",
    "    return beta\n",
    "\n",
    "\n",
    "## Optimize alpha\n",
    "#  (Newton-Raphson method, for a Hessian with special structure)\n",
    "def est_alpha(alpha,gamma,M,k,nr_max_iters = 1000,tol = 10**-2.0):\n",
    "    for it in range(nr_max_iters):\n",
    "        alpha_old = alpha\n",
    "        \n",
    "        #  Calculate gradient \n",
    "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) + np.sum(digamma(gamma)-np.tile(digamma(np.sum(gamma,axis=1)),(k,1)).T,axis=0)\n",
    "        #  Calculate Hessian diagonal component\n",
    "        h = -M*polygamma(1,alpha) \n",
    "        #  Calculate Hessian constant component\n",
    "        z = M*polygamma(1,np.sum(alpha))\n",
    "        #  Calculate constant\n",
    "        c = np.sum(g/h)/(z**(-1.0)+np.sum(h**(-1.0)))\n",
    "\n",
    "        #  Update alpha\n",
    "        alpha = alpha - (g-c)/h\n",
    "        \n",
    "        #  Check convergence\n",
    "        if sqrt(mean(square(alpha-alpha_old)))<tol:\n",
    "            break\n",
    "        \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization - Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the preformance of Gibbs Sampling, in this section, we use Cython to speed the algorithm up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import cython\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "\n",
    "def gibbs_sampling_cy(corpus, int max_iter, int K, int V, n_zw, n_z, n_mz, n_m, z_mw, double alpha, double phi):\n",
    "    \n",
    "    cdef int i, m, n, w, t\n",
    "\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    def sample_topic(int K, n_zw, n_z, n_mz, n_m, double alpha, double phi, int w, int m):\n",
    "        \"\"\"\n",
    "        Sample new topic for current word\n",
    "\n",
    "        \"\"\"\n",
    "        p_z = np.zeros(K)\n",
    "        cdef int j\n",
    "        for j in range(K):\n",
    "            p_z[j] = ((n_zw[j, w] + phi)/(n_z[j] + V * phi)) * ((n_mz[m, j] + alpha)/(n_m[m] + K * alpha))\n",
    "        new_z = np.random.multinomial(1, p_z/p_z.sum()).argmax()\n",
    "        return new_z  \n",
    "\n",
    "    \n",
    "    def update_beta(int V, n_zw, n_z, double alpha):\n",
    "        \"\"\"\n",
    "        Update beta\n",
    "        \"\"\"\n",
    "        beta = (n_zw + alpha)/(n_z[:,None] + V *alpha)\n",
    "        return beta\n",
    "\n",
    "    def update_theta(int K, n_mz, n_m, double phi):\n",
    "        \"\"\"\n",
    "        Update theta\n",
    "        \"\"\"\n",
    "        theta = (n_mz + phi)/(n_m[:, None] + K * phi)\n",
    "        return theta\n",
    "\n",
    "    beta_gibbs = []\n",
    "    theta_gibbs = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        for m, doc in enumerate(corpus):\n",
    "            for n, (w, t) in enumerate(doc):\n",
    "                #exclude the current word\n",
    "                z = z_mw[m][n]\n",
    "                n_mz[m, z] -= t\n",
    "                n_m[m] -= t\n",
    "                n_zw[z, w] -= t\n",
    "                n_z[z] -= t\n",
    "        \n",
    "                new_z = sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m)\n",
    "\n",
    "                #include the current word\n",
    "                z_mw[m][n] = new_z\n",
    "                n_mz[m, new_z] += t\n",
    "                n_zw[new_z, w] += t\n",
    "                n_z[new_z] += t\n",
    "                n_m[m] += t\n",
    "\n",
    "        #update beta\n",
    "        beta_gibbs.append(update_beta(V, n_zw, n_z, alpha))\n",
    "        #update theta\n",
    "        theta_gibbs.append(update_theta(K, n_mz, n_m, phi))\n",
    "    return beta_gibbs, theta_gibbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Tests - Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data set characteristics\n",
    "To test if our implementation of latent dirichlet allocation with variational inference works, we first generate some toy data. This toy data set will consist of 300 documents, each with a uniform random length between 150 and 200 words. The size of the vocabulary of words in the documents is set to be 30, assumed to be generated from 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "M = 300\n",
    "k = 10\n",
    "N = np.random.randint(150,200,size=M)\n",
    "V = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "The documents are then generated one by one according to the LDA model (see 2 Algorithm description). Three distinct groups of documents are generated: the first 100 have a strong preference for topics 1, 2, and 3; the second 100 have a strong preference for topics 4, 5, and 6; and the last 100 have a strong preference for topics 7, 8, 9, and 10. Furthermore, each topic will have a strong preference for 3 words, such that each word is prevalent in one topic. The structure of the resulting parameters are shown in Figures 1 and 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3 groups of documents, each with a topic preference\n",
    "alpha_gen1 = np.array((20,15,10,1,1,1,1,1,1,1))\n",
    "alpha_gen2 = np.array((1,1,1,10,15,20,1,1,1,1))\n",
    "alpha_gen3 = np.array((1,1,1,1,1,1,10,12,15,18))\n",
    "\n",
    "# Arbitrarily choose each topic to have 3 very common words\n",
    "beta_probs = np.ones((V,k)) + np.array([np.arange(V)%k==i for i in range(k)]).T*19\n",
    "beta_gen = np.array(list(map(lambda x: np.random.dirichlet(x),beta_probs.T))).T\n",
    "\n",
    "w_struct = list();\n",
    "theta = np.empty((M,k))\n",
    "\n",
    "# Generate each document\n",
    "for m in range(M):\n",
    "    # Draw topic distribution for the document\n",
    "    if m<M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen1,1)[0]\n",
    "    elif m<2*M/3:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen2,1)[0]\n",
    "    else:\n",
    "        theta[m,:] = np.random.dirichlet(alpha_gen3,1)[0]\n",
    "    doc = np.array([])\n",
    "    \n",
    "    for n in range(N[m]):\n",
    "        # Draw topic according to document's topic distribution\n",
    "        z_n = np.random.choice(np.arange(k),p=theta[m,:])\n",
    "        # Draw word according to topic\n",
    "        w_n = np.random.choice(np.arange(V),p=beta_gen[:,z_n])\n",
    "        doc = np.append(doc,w_n)\n",
    "    w_struct.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_gen](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_gen.png?raw=true)\n",
    "\n",
    "<center>Figure 1: The matrix $\\beta_{gen}$ used to set the topic-word probabilities of the test data. Note how each topic strongly prefers 3 words.</center>\n",
    "\n",
    "![theta_gen](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/theta.png?raw=true)\n",
    "\n",
    "<center>Figure 2: The matrix $\\theta$ generated by the model. Note how there are 3 distinct groups, each with their preferred topics.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters $\\alpha, \\beta, \\phi$ and $\\gamma$\n",
    "The model and variational parameters are then randomly initialized to reasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 100*np.random.dirichlet(10*np.ones(k),1)[0]\n",
    "beta = np.random.dirichlet(np.ones(V),k).T\n",
    "\n",
    "phi = np.array([1/k*np.ones([N[m],k]) for m in range(M)])\n",
    "gamma = np.tile(alpha,(M,1)) + np.tile(N/k,(k,1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Criterion\n",
    "The variational inference parameter $\\gamma$ contains the topic likelihoods of every document. As such, $\\gamma$ identifies to which group a document is likely to belong. As such, the convergence criterion was chosen to monitor this parameter. The root-mean-square of the change in $\\gamma$ is calculated on every iteration of EM and compared against a tolerance parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def converged(gamma,gamma_old,convergence):\n",
    "    #print(sqrt(mean(square(gamma-gamma_old))))\n",
    "    return sqrt(mean(square(gamma-gamma_old))) < convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference by iterative EM\n",
    "\n",
    "Expectation-Maximization is carried out by consecutively maximizing each of the four parameters $\\alpha, \\beta, \\phi$ and $\\gamma$ with respect to the log likelihood until either the convergence criterion has been met or a maximimum number of iterations have been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrea/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "convergence = 5*10**(-2.0)\n",
    "successfully_Converged = False\n",
    "max_iters = 10**3\n",
    "\n",
    "for iters in range(max_iters):\n",
    "    #print(iters)\n",
    "    gamma_old = gamma\n",
    "    \n",
    "    ## Expectation step: Update variational parameters\n",
    "    phi   = opt_phi(beta,gamma,w_struct,M,N,k)\n",
    "    gamma = opt_gamma(alpha,phi,M)\n",
    "    \n",
    "    ## Maximization step: Update model parameters\n",
    "    beta  = est_beta(phi,w_struct,k,V)\n",
    "    alpha = est_alpha(alpha,gamma,M,k)\n",
    "    \n",
    "    if converged(gamma,gamma_old,convergence):\n",
    "        successfully_Converged = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After running until the root-mean-square of the difference in $\\gamma$ dropped below 0.05, the algorithm was considered converged and terminated. The results are visualized in plots below in Figures 3 and 4. Since the model parameter $\\beta$ estimated by the algorithm should correspond witht the $\\beta$ used to generate the data, and inferred variational parameter $\\gamma$ should correspond to $\\theta$, Figures 3 and 4 should resemble Figures 1 and 2 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_est_unordered](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_est_unordered.png?raw=true)\n",
    "\n",
    "<center>Figure 3: The matrix $\\beta_{est}$ inferred by the algorithm.</center>\n",
    "\n",
    "![gamma_unordered](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/gamma_unordered.png?raw=true)\n",
    "\n",
    "<center>Figure 4: The matrix $\\gamma$ inferred by the algorithm.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, Figures 1 and 2 do not appear to match up with Figures 3 and 4. However, the individual topic identities do not have any specific relation to their index. In other words, there is a non-identifiability issue at play here. It is however apparent in Figure 4 that the algorithm correctly identifies 3 separate groups of documents of the right size. By visually inspecting $\\gamma$ and re-arranging the order of the topics, we get Figures 5 and 6 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beta_est_rearranged](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/beta_est_rearranged.png?raw=true)\n",
    "\n",
    "<center>Figure 5: The matrix $\\beta_{est}$ inferred by the algorithm, rearranged in original topic order.</center>\n",
    "\n",
    "![gamma_rearranged](https://github.com/gytcrt/FinalProject663/blob/master/Output_Data/gamma_rearranged.png?raw=true)\n",
    "\n",
    "<center>Figure 6: The matrix $\\gamma$ inferred by the algorithm, rearranged in original topic order.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect, the visualizations of $\\beta$ and $\\gamma$ with the topics re-arranged now exhibit patterns similar to the original structures visible in Figures 1 and 2. In particular, the three diagonals in $\\beta$ representing the three preferred words of each topic can be clearly seen in Figure 5, and the three boxes corresponding to the preferred topic distributions of the three groups of documents are also apparent in Figure 6. With more data (either more documents or more words in each document), these structures are likely to be recovered with even higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Tests - Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test Gibbs Sampling and compare the performance of the Cython version, we use NLTK, stop_words, gensim packages in Python to clean our toy corpus. The cleaning processes are:\n",
    "\n",
    "* tokenizing: remove punctuation and split into individual words\n",
    "* stop words: remove common words like \"the\", \"a\", \"I\", and so on\n",
    "* stemming: reduce derived or inflected words to their word stem. For example, \"stems\", \"stemmer\", \"stemming\", and \"stemmed\" are based on \"stem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Batman became popular soon after his introduction and gained his own comic book title, Batman, in 1940.\"\n",
    "\n",
    "doc_b = \"In 1971, Trump moved to Manhattan, where he became involved in larger construction projects, and used attractive architectural design to win public recognition.\"\n",
    "\n",
    "doc_c = \"Batman is, in his everyday identity, Bruce Wayne, a wealthy American business magnate living in Gotham City.\"\n",
    "\n",
    "doc_d = \"In 2001, Trump completed Trump World Tower, a 72-story residential tower across from the United Nations Headquarters.\"\n",
    "\n",
    "doc_e = \" Unlike most superheroes, Batman does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. \" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)  \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize prior and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the corpus, there are 2 topics: Batman and Donald Trump, and the vocabulary contains 69 words. To fit the LDA model to the corpus, we set the priors $\\alpha$ and $\\phi$ to 0.5, and run the Gibbs Sampler for 1000 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus : corpus contains bag-of-words\n",
    "#K : number of topics\n",
    "#V : vocaburary size\n",
    "\n",
    "K = 2\n",
    "V = 69\n",
    "alpha = 0.5\n",
    "phi = 0.5\n",
    "max_iter = 1000\n",
    "\n",
    "#intialize parameters\n",
    "n_m = words_count_doc(corpus)\n",
    "z_mw, n_mz, n_zw, n_z = initial_parameters(corpus, K, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def timer(f, *args, **kwargs):\n",
    "    start = time.clock()\n",
    "    ans = f(*args, **kwargs)\n",
    "    return ans, time.clock() - start\n",
    "def report(fs, *args, **kwargs):\n",
    "    ans, t = timer(fs[0], *args, **kwargs)\n",
    "    print('%s: %.1f' % (fs[0].__name__, 1.0))  \n",
    "    for f in fs[1:]:\n",
    "        ans_, t_ = timer(f, *args, **kwargs)\n",
    "        print('%s: %.1f' % (f.__name__, t/t_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "gibbs_sampling: 1.0\n",
      "gibbs_sampling_cy: 1.2\n"
     ]
    }
   ],
   "source": [
    "report([gibbs_sampling, gibbs_sampling_cy], corpus, max_iter, K, V, n_zw, n_z, n_mz, n_m, z_mw, alpha, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "After running Gibbs Samping 1000 times for the original version and the Cython version and timing them, we can conclude Cython compilation improved the performance of Gibbs Sampling by 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented in Python via Gibbs Sampling and Variational Inference. Using the Dirchlet distribution as a prior, the goal was to discover latent topics among documents in a corpus. We simulated a dataset according to the generative process in Blei et al's paper. We then used LDA to estimate the latent variables and compared them with real latent variables in the generating process. As the visualization of latent variables showed, the estimated values were quite close to the real values. Also, we tried to improve the performance of our Gibbs sampling method with Cython, and it sped up the algorithm by 20%. In the future, we can further optimize our algorithm, as we did not tailor the data structure for Cython implementation. Once we do that, the perfomance of the algorithm will likely be boosted to a large degree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n",
    "\n",
    "[2] Griffiths, Tom. \"Gibbs sampling in the generative model of latent dirichlet allocation.\" (2002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
